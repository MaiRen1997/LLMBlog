---
title: python调用ollama本地大模型
date: 2026-02-24 11:02:39
permalink: /pages/f70899/
categories:
  - Agent学习
  - ollama的使用
tags:
  - 
---
## 安装langchain-ollama插件

1. 使用uv进行项目级的安装

   ```sh
   uv add langchain-ollama
   ```

## 使用插件

### [具体可以参照文档](https://docs.langchain.com/oss/python/integrations/providers/ollama)

### 使用对话模型

1. model的值，是本地安装的大模型的名字

```python
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.1",
    temperature=0,
    # other params...
)
resp = llm.invoke("你是谁？")
print(resp)
```

## 使用流式调用

1. 将上面的`invoke` 替换成 `stream`

   ```python
   resp = llm.stream("你是谁？")
   for chunk in resp:
       print(chunk.content, end="")
   ```

   1. resp 是一个数组，它会不断接收新的内容
   2. 由于print每打印一个，会有一个换行，使用end 参数，将默认的\n换成空字符串即可